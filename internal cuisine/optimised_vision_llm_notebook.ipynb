{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Vision-LLM Zero to Hero: Optimized Fine-Tuning (QLoRA)\n",
        "\n",
        "## Introduction\n",
        "Ce notebook est la solution **\"Best Ever\"** pour entra√Æner un mod√®le Vision-LLM (comme **Qwen-VL** ou **BLIP-2**) sur le dataset RAF-CE sans avoir besoin d'un supercalculateur.\n",
        "\n",
        "### Pourquoi cette approche ?\n",
        "1.  **Vision-LLM (SOTA)** : Contrairement aux CNN (ResNet) ou ViT classiques, ce mod√®le *comprend* l'image et peut expliquer pourquoi il voit une √©motion compos√©e.\n",
        "2.  **QLoRA (4-bit Quantization)** : Nous allons charger le mod√®le en **4-bits** (compression extr√™me) et n'entra√Æner que des petits adaptateurs (**LoRA**). \n",
        "    *   *R√©sultat* : Entra√Ænement possible sur un GPU grand public (T4/L4/A10) en quelques heures au lieu de jours.\n",
        "3.  **Qualit√© Professionnelle** : Code modulaire, gestion des erreurs, et pipeline de donn√©es robuste.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup \"Zero Config\" üõ†Ô∏è\n",
        "Installation automatique de toutes les biblioth√®ques n√©cessaires optimis√©es (bitsandbytes, peft, transformers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"torch torchvision torchaudio\",\n",
        "        \"transformers>=4.37.0\",\n",
        "        \"peft\",\n",
        "        \"bitsandbytes\",\n",
        "        \"accelerate\",\n",
        "        \"datasets\",\n",
        "        \"pillow\",\n",
        "        \"scikit-learn\",\n",
        "        \"scipy\",\n",
        "        \"tensorboard\"\n",
        "    ]\n",
        "    print(\"‚ö° Installing optimized libraries for QLoRA...\")\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + package.split())\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning installing {package}: {e}\")\n",
        "    print(\"‚úÖ Environment Ready!\")\n",
        "\n",
        "# Uncomment to run installation\n",
        "# install_dependencies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Force Cuda if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üöÄ Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration du Mod√®le (The \"Smart\" Part) üß†\n",
        "Nous allons utiliser **Qwen-VL-Chat** (ou une alternative comme BLIP-2). Qwen-VL est actuellement l'un des meilleurs mod√®les open-source pour la compr√©hension visuelle pr√©cise.\n",
        "\n",
        "**Magie QLoRA** :\n",
        "*   `load_in_4bit=True` : Divise par 4 la m√©moire requise.\n",
        "*   `bcd_quant_type=\"nf4\"` : Type de donn√©es normalis√© pour ne pas perdre en pr√©cision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen-VL-Chat-Int4\"  # Version optimis√©e 4-bit native de Qwen\n",
        "# Alternative si Qwen est trop lourd: \"Salesforce/blip2-opt-2.7b\"\n",
        "\n",
        "def load_model_and_processor():\n",
        "    print(f\"üîÑ Loading {MODEL_ID}...\")\n",
        "    \n",
        "    # 1. Load Tokenizer & Processor\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    \n",
        "    # 2. 4-Bit Configuration (QLoRA)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16, # Use float16 for speed\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    # 3. Load Base Model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # 4. Prepare for Training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # 5. Apply LoRA Adapters\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,               # Rank (higher = smarter but heavier)\n",
        "        lora_alpha=32,      # Scaling factor\n",
        "        target_modules=[\"c_attn\", \"attn.c_proj\", \"w1\", \"w2\"], # Targeted Linear Layers\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters() # Show how efficient we are!\n",
        "    \n",
        "    return model, processor, tokenizer\n",
        "\n",
        "# model, processor, tokenizer = load_model_and_processor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading Intelligent üìÇ\n",
        "Nous transformons les images et les labels en **Conversations**.\n",
        "\n",
        "**Format du Dataset Vision-LLM** :\n",
        "*   **User** : `<Image> Analyze the facial expression. What is the compound emotion?`\n",
        "*   **Assistant** : `The person is Happily Surprised. Facial cues: raised eyebrows, smiling mouth.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAFCE_LLM_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_list, processor, tokenizer):\n",
        "        self.data = data_list\n",
        "        self.processor = processor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_path = item[\"path\"]\n",
        "        label_text = item[\"label_text\"] # e.g., \"Happily Surprised\"\n",
        "        explanation = item.get(\"explanation\", \"Facial features align with this emotion.\") # Add logic here if you have AU data\n",
        "\n",
        "        # 1. Create Prompt\n",
        "        prompt = f\"User: <img>{img_path}</img> Analyze the facial expression. What is the compound emotion?\\nAssistant: The emotion is {label_text}. {explanation}<|endoftext|>\"\n",
        "        \n",
        "        # 2. Process Image & Text using Qwen's specific method\n",
        "        # Note: This part depends highly on the specific model's API (Qwen vs BLIP)\n",
        "        # Here is a generic wrapper for Qwen-VL\n",
        "        \n",
        "        inputs = self.processor(\n",
        "            text=[prompt],\n",
        "            images=None, # Qwen handles image path inside text\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": inputs[\"input_ids\"].squeeze() # Causal LM training (predict next token)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop Optimis√©e üî•\n",
        "Utilisation de `transformers.Trainer` avec des param√®tres optimis√©s pour ne pas gaspiller de temps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def run_training(model, train_dataset, val_dataset):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./qwen_rafce_finetuned\",\n",
        "        per_device_train_batch_size=4,    # Low batch size because model is huge\n",
        "        gradient_accumulation_steps=8,    # Accumulate gradients to simulate batch_size=32\n",
        "        num_train_epochs=3,               # Vision-LLMs learn FAST. 3 epochs is often enough.\n",
        "        learning_rate=2e-4,               # LoRA allows higher LR than full fine-tuning\n",
        "        bf16=True,                        # Use bfloat16 for stability if hardware supports it\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        report_to=\"tensorboard\",\n",
        "        remove_unused_columns=False       # Important for custom multimodal datasets\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        # DataCollator would be needed here for complex padding\n",
        "    )\n",
        "    \n",
        "    print(\"üî• Starting QLoRA Fine-Tuning...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"‚úÖ Training Complete. Saving Adapters...\")\n",
        "    model.save_pretrained(\"./best_vision_llm_adapter\")\n",
        "\n",
        "# To run:\n",
        "# 1. Prepare data_list from RAFCE files (same logic as before but with text labels)\n",
        "# 2. dataset = RAFCE_LLM_Dataset(data_list, processor, tokenizer)\n",
        "# 3. run_training(model, dataset, val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inf√©rence & D√©mo (The \"Wow\" Factor) ‚ú®\n",
        "Une fonction simple pour tester le mod√®le entra√Æn√© sur une nouvelle image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_emotion(model, tokenizer, processor, image_path):\n",
        "    prompt = f\"User: <img>{image_path}</img> Analyze this face. What is the emotion?\\nAssistant:\"\n",
        "    \n",
        "    inputs = processor(text=[prompt], return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False, # Deterministic for evaluation\n",
        "        temperature=0.0  # Greedy decoding\n",
        "    )\n",
        "    \n",
        "    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return output_text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "# demo_img = \"test_image.jpg\"\n",
        "# print(predict_emotion(model, tokenizer, processor, demo_img))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
