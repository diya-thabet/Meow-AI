{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 4: Vision-LLM XAI & Evaluation\n",
                "\n",
                "**Goal**: Interpret the Vision-LLM with attention visualization and evaluate text generation quality.\n",
                "\n",
                "**Key Practices**:\n",
                "- Extract attention maps from the Vision Encoder\n",
                "- BLEU/ROUGE scores for text quality\n",
                "- Compare ResNet Grad-CAM vs VLM Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q evaluate nltk matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "from tqdm.auto import tqdm\n",
                "import evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "DATA_DIR = \"/kaggle/input/processed-rafce/processed_dataset\"\n",
                "VLM_MODEL_DIR = \"/kaggle/input/vlm-lora/lora_model\"  # From Notebook 3\n",
                "OUTPUT_DIR = \"/kaggle/working/xai_output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the fine-tuned model\n",
                "from unsloth import FastVisionModel\n",
                "\n",
                "model, tokenizer = FastVisionModel.from_pretrained(\n",
                "    model_name=VLM_MODEL_DIR,\n",
                "    max_seq_length=2048,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "\n",
                "FastVisionModel.for_inference(model)\n",
                "print(\"Model loaded for inference\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Text Evaluation (BLEU/ROUGE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load evaluation metrics\n",
                "rouge = evaluate.load(\"rouge\")\n",
                "bleu = evaluate.load(\"bleu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create test dataset with ground truth\n",
                "def create_test_samples(data_dir, au_labels_file, num_samples=50):\n",
                "    \"\"\"Create test samples with ground truth explanations.\"\"\"\n",
                "    # Load AU labels\n",
                "    au_labels = {}\n",
                "    with open(au_labels_file, 'r') as f:\n",
                "        for line in f:\n",
                "            parts = line.strip().split()\n",
                "            if len(parts) >= 2:\n",
                "                au_labels[parts[0]] = parts[1] if parts[1] != 'null' else ''\n",
                "    \n",
                "    AU_DESCRIPTIONS = {\n",
                "        '1': 'Inner Brow Raiser', '2': 'Outer Brow Raiser', '4': 'Brow Lowerer',\n",
                "        '6': 'Cheek Raiser', '12': 'Lip Corner Puller (Smile)', '25': 'Lips Part'\n",
                "    }\n",
                "    \n",
                "    samples = []\n",
                "    test_dir = Path(data_dir) / 'test'\n",
                "    \n",
                "    for emotion_dir in test_dir.iterdir():\n",
                "        if not emotion_dir.is_dir():\n",
                "            continue\n",
                "        \n",
                "        emotion_name = emotion_dir.name.replace('_', ' ')\n",
                "        \n",
                "        for img_path in list(emotion_dir.glob('*.jpg'))[:5]:  # 5 per class\n",
                "            base_name = img_path.stem.replace('_aligned', '') + '.jpg'\n",
                "            au_string = au_labels.get(base_name, '')\n",
                "            \n",
                "            # Create ground truth\n",
                "            if au_string:\n",
                "                aus = au_string.replace('+', ' ').split()\n",
                "                au_text = ', '.join([AU_DESCRIPTIONS.get(au, f'AU{au}') for au in aus])\n",
                "                reference = f\"The emotion is {emotion_name}. Visible facial cues include: {au_text}.\"\n",
                "            else:\n",
                "                reference = f\"The emotion is {emotion_name}.\"\n",
                "            \n",
                "            samples.append({\n",
                "                'image_path': str(img_path),\n",
                "                'emotion': emotion_name,\n",
                "                'reference': reference\n",
                "            })\n",
                "    \n",
                "    return samples[:num_samples]\n",
                "\n",
                "AU_LABELS_FILE = \"/kaggle/input/raf-au/RAFCE_AUlabel.txt\"\n",
                "test_samples = create_test_samples(DATA_DIR, AU_LABELS_FILE)\n",
                "print(f\"Created {len(test_samples)} test samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference and collect predictions\n",
                "def run_inference(model, tokenizer, image_path, prompt):\n",
                "    \"\"\"Run inference on a single image.\"\"\"\n",
                "    image = Image.open(image_path).convert('RGB')\n",
                "    \n",
                "    messages = [\n",
                "        {'role': 'user', 'content': [\n",
                "            {'type': 'image'},\n",
                "            {'type': 'text', 'text': prompt}\n",
                "        ]}\n",
                "    ]\n",
                "    \n",
                "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
                "    inputs = tokenizer(\n",
                "        image,\n",
                "        input_text,\n",
                "        add_special_tokens=False,\n",
                "        return_tensors='pt'\n",
                "    ).to('cuda')\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=256,\n",
                "            use_cache=True,\n",
                "            temperature=0.7,\n",
                "        )\n",
                "    \n",
                "    # Extract only the generated part\n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Try to extract assistant response\n",
                "    if 'assistant' in response.lower():\n",
                "        response = response.split('assistant')[-1].strip()\n",
                "    return response\n",
                "\n",
                "prompt = \"Analyze the facial expression. Classify the compound emotion and explain which facial cues led to this conclusion.\"\n",
                "predictions = []\n",
                "references = []\n",
                "\n",
                "for sample in tqdm(test_samples, desc=\"Generating predictions\"):\n",
                "    try:\n",
                "        pred = run_inference(model, tokenizer, sample['image_path'], prompt)\n",
                "        predictions.append(pred)\n",
                "        references.append(sample['reference'])\n",
                "    except Exception as e:\n",
                "        print(f\"Error on {sample['image_path']}: {e}\")\n",
                "        predictions.append(\"\")\n",
                "        references.append(sample['reference'])\n",
                "\n",
                "print(f\"\\nGenerated {len(predictions)} predictions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate BLEU and ROUGE\n",
                "# Filter out empty predictions\n",
                "valid_preds = [(p, r) for p, r in zip(predictions, references) if p]\n",
                "pred_list = [p for p, r in valid_preds]\n",
                "ref_list = [[r] for p, r in valid_preds]  # BLEU expects list of references\n",
                "\n",
                "# BLEU Score\n",
                "bleu_result = bleu.compute(predictions=pred_list, references=ref_list)\n",
                "print(f\"BLEU Score: {bleu_result['bleu']:.4f}\")\n",
                "\n",
                "# ROUGE Scores\n",
                "rouge_result = rouge.compute(predictions=pred_list, references=[r[0] for r in ref_list])\n",
                "print(f\"ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
                "print(f\"ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
                "print(f\"ROUGE-L: {rouge_result['rougeL']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show sample predictions\n",
                "print(\"\\n=== Sample Predictions ===\")\n",
                "for i in range(min(5, len(valid_preds))):\n",
                "    print(f\"\\n--- Sample {i+1} ---\")\n",
                "    print(f\"Reference: {ref_list[i][0]}\")\n",
                "    print(f\"Prediction: {pred_list[i]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Vision Encoder Attention Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract attention from Vision Encoder\n",
                "# Note: This is model-specific. For Qwen2-VL, we access the vision encoder attention.\n",
                "\n",
                "def get_vision_attention(model, tokenizer, image_path):\n",
                "    \"\"\"Extract attention maps from the vision encoder.\"\"\"\n",
                "    image = Image.open(image_path).convert('RGB')\n",
                "    \n",
                "    messages = [\n",
                "        {'role': 'user', 'content': [\n",
                "            {'type': 'image'},\n",
                "            {'type': 'text', 'text': 'Describe this image.'}\n",
                "        ]}\n",
                "    ]\n",
                "    \n",
                "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
                "    inputs = tokenizer(\n",
                "        image,\n",
                "        input_text,\n",
                "        add_special_tokens=False,\n",
                "        return_tensors='pt'\n",
                "    ).to('cuda')\n",
                "    \n",
                "    # Forward pass with attention output\n",
                "    with torch.no_grad():\n",
                "        outputs = model(\n",
                "            **inputs,\n",
                "            output_attentions=True,\n",
                "            return_dict=True\n",
                "        )\n",
                "    \n",
                "    # Get attention from the last layer\n",
                "    # Note: Structure depends on model architecture\n",
                "    attentions = outputs.attentions if hasattr(outputs, 'attentions') else None\n",
                "    \n",
                "    return attentions, image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_attention_simple(image_path, attention_map, output_path):\n",
                "    \"\"\"Visualize attention map overlay on image.\"\"\"\n",
                "    # Load and resize image\n",
                "    img = Image.open(image_path).convert('RGB')\n",
                "    img = img.resize((336, 336))\n",
                "    img_array = np.array(img)\n",
                "    \n",
                "    # Create figure\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
                "    \n",
                "    # Original image\n",
                "    axes[0].imshow(img_array)\n",
                "    axes[0].set_title('Original Image')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Attention heatmap\n",
                "    if attention_map is not None:\n",
                "        # Reshape attention to 2D grid (depends on patch size)\n",
                "        h = w = int(np.sqrt(attention_map.shape[-1]))\n",
                "        attn_resized = attention_map.reshape(h, w)\n",
                "        attn_resized = np.array(Image.fromarray(attn_resized).resize((336, 336)))\n",
                "        \n",
                "        axes[1].imshow(attn_resized, cmap='jet')\n",
                "        axes[1].set_title('Attention Map')\n",
                "        axes[1].axis('off')\n",
                "        \n",
                "        # Overlay\n",
                "        axes[2].imshow(img_array)\n",
                "        axes[2].imshow(attn_resized, cmap='jet', alpha=0.5)\n",
                "        axes[2].set_title('Attention Overlay')\n",
                "        axes[2].axis('off')\n",
                "    else:\n",
                "        axes[1].text(0.5, 0.5, 'Attention not available', ha='center', va='center')\n",
                "        axes[2].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(output_path)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: Full attention extraction may require model-specific hooks.\n",
                "# This is a simplified example that may need adjustment based on the actual model.\n",
                "\n",
                "print(\"\\n=== Attention Visualization ===\")\n",
                "print(\"Note: Full Vision Encoder attention extraction requires model-specific hooks.\")\n",
                "print(\"For Qwen2-VL, you may need to access model.visual.blocks[-1].attn\")\n",
                "print(\"\\nFor a complete implementation, consider using:\")\n",
                "print(\"- BertViz for transformer attention visualization\")\n",
                "print(\"- Custom forward hooks to capture intermediate activations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save evaluation results\n",
                "results = {\n",
                "    'bleu': bleu_result['bleu'],\n",
                "    'rouge1': rouge_result['rouge1'],\n",
                "    'rouge2': rouge_result['rouge2'],\n",
                "    'rougeL': rouge_result['rougeL'],\n",
                "    'num_samples': len(valid_preds)\n",
                "}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, 'evaluation_results.json'), 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(\"\\n=== Evaluation Summary ===\")\n",
                "print(f\"BLEU Score: {results['bleu']:.4f}\")\n",
                "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
                "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
                "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
                "print(f\"Samples evaluated: {results['num_samples']}\")\n",
                "print(f\"\\nResults saved to {OUTPUT_DIR}/evaluation_results.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}