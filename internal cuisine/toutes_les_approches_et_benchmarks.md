# Rapport Complet : Vision-LLM pour la Reconnaissance Faciale des √âmotions Compos√©es (FER-CE)

## 1. Contexte et Motivation

### Le Probl√®me : Au-del√† des √âmotions Simples
La reconnaissance faciale des √©motions (FER) est un domaine cl√© de l'intelligence artificielle, utilis√© en psychologie, robotique et interaction homme-machine.

Historiquement, les syst√®mes classiques (utilisant des r√©seaux CNN comme ResNet) se concentraient sur **7 √©motions basiques** : joie, col√®re, tristesse, peur, d√©go√ªt, surprise et neutre.

Cependant, dans la vraie vie, les humains sont bien plus complexes. Nous ressentons souvent des **√©motions compos√©es** (Compound Expressions), c'est-√†-dire un m√©lange de deux √©motions simultan√©es.
Par exemple :
*   **Happily Surprised** (Heureusement surpris) : Yeux grands ouverts (surprise) + Sourire (joie).
*   **Sadly Angry** (Tristement en col√®re) : Un m√©lange amer de frustration et de peine.
*   **Fearfully Disgusted** (D√©go√ªt√© et effray√©).

Ces √©motions mixtes sont tr√®s difficiles √† d√©tecter pour les IA classiques car les signaux sur le visage (les micro-mouvements musculaires appel√©s AUs - Action Units) sont subtils et parfois contradictoires.

### La Solution : Vision-LLM
C'est l√† qu'interviennent les **Vision-LLMs** (Large Vision-Language Models). Ces mod√®les r√©volutionnaires ne se contentent pas de "voir" une image, ils peuvent la "comprendre" et en parler comme un humain.

L'objectif de ce projet est d'utiliser un Vision-LLM non seulement pour **classifier** ces √©motions complexes (dire "c'est de la tristesse m√™l√©e √† de la col√®re"), mais aussi pour **expliquer pourquoi** (dire "Je vois des sourcils fronc√©s typiques de la col√®re, mais des yeux tombants qui marquent la tristesse").

---

## 2. Donn√©es Utilis√©es : Le Dataset RAF-CE

Pour ce projet, nous utilisons le jeu de donn√©es **RAF-CE** (Real-world Affective Faces - Compound Expressions).

*   **Contenu** : Des images de visages en conditions r√©elles (pas d'acteurs en studio, mais des vraies photos du web).
*   **Classes** : Il contient **14 cat√©gories** d'√©motions compos√©es.
*   **Richesse** : Chaque image poss√®de aussi des annotations sur les mouvements musculaires (Action Units), ce qui nous aide √† comprendre la m√©canique du visage.

---

## 3. M√©thodologie : Notre Pipeline en 3 Couches

Nous avons con√ßu une approche structur√©e en trois √©tapes pour r√©soudre ce probl√®me.

### Couche 1 : Pr√©paration des Donn√©es
Avant de nourrir l'IA, nous devons pr√©parer les images :
1.  **D√©tection et Recadrage** : On s'assure que le visage est bien au centre.
2.  **Normalisation** : On ajuste les couleurs et la lumi√®re pour que tout soit coh√©rent.
3.  **Data Augmentation** : On cr√©e des variantes des images (rotations l√©g√®res, changement de luminosit√©) pour rendre le mod√®le plus robuste et √©viter qu'il n'apprenne par c≈ìur.

### Couche 2 : Le C≈ìur Vision-LLM
Ici, nous combinons la vision et le langage.
*   **L'≈ìil (Encodeur Visuel)** : On utilise des mod√®les puissants comme CLIP ou ViT pour analyser les pixels.
*   **Le Cerveau (LLM)** : On utilise un mod√®le de langage (comme Vicuna ou LLaMA) pour raisonner.
*   **Le Lien (Q-Former)** : C'est le pont qui traduit ce que l'≈ìil et voit en concepts que le cerveau peut comprendre.

**Objectifs d'apprentissage :**
1.  **Classification** : Pr√©dire correctement l'une des 14 √©motions compos√©es.
2.  **Explication** : G√©n√©rer une phrase qui d√©crit l'√©motion (ex: "La personne semble agr√©ablement surprise, ses yeux sont √©carquill√©s et elle sourit.").

**Technique Avanc√©e : Prompt Engineering Visuel**
Nous guidons le mod√®le avec des instructions pr√©cises, par exemple :
> *"D√©cris l'√©tat √©motionnel et explique quels indices faciaux y contribuent (sourcils, bouche, yeux)."*
Cela force le mod√®le √† √™tre attentif aux d√©tails physiques.

### Couche 3 : Interpr√©tation Multimodale (Comprendre la d√©cision)
Il ne suffit pas que l'IA ait raison, il faut savoir pourquoi.
*   **Visuellement (Grad-CAM)** : Nous g√©n√©rons des cartes de chaleur (heatmaps) pour voir o√π l'IA regarde. Regarde-t-elle bien la bouche pour un sourire ? Ou se perd-t-elle sur le fond de l'image ?
*   **Linguistiquement** : Nous analysons les phrases g√©n√©r√©es pour v√©rifier si elles sont coh√©rentes avec l'image.

---

## 4. Benchmarks et R√©sultats Exp√©rimentaux

Nous avons compar√© plusieurs approches pour √©valuer la performance de notre solution.

### 4.1. Approches Vision-Only (Baselines)
Nous avons d'abord test√© des mod√®les classiques de vision par ordinateur pour √©tablir un score de r√©f√©rence.

1.  **ResNet-50** (Test√© dans `Ala's Try` et `Sat Try`)
    *   Architecture robuste et √©prouv√©e.
    *   **R√©sultat obtenu** : ~51% d'Accuracy.
    *   *Observation* : Le mod√®le peine √† distinguer les nuances subtiles entre deux √©motions proches.
2.  **ViT (Vision Transformer)** (Explor√© dans `Dhia Try`)
    *   D√©coupe l'image en "patches" et analyse les relations globales.
    *   Potentiellement plus puissant que ResNet sur des grands datasets, mais demande beaucoup de donn√©es pour converger.

### 4.2. Approches Vision-LLM (Notre Innovation)
Nous proposons l'utilisation de mod√®les multimodaux :
*   **BLIP-2 / LLaVA / Qwen-VL**
*   **Avantages attendus** :
    *   Meilleure compr√©hension du contexte global.
    *   Capacit√© √† utiliser la connaissance du langage pour d√©sambigu√Øser des expressions visuelles complexes.
    *   **Score vis√©** : Sup√©rieur aux 51% du ResNet, avec en prime la capacit√© d'explication.

### Tableau Comparatif des Performances
| Mod√®le | Type | Accuracy (Est.) | Avantages | Inconv√©nients |
| :--- | :--- | :--- | :--- | :--- |
| **ResNet-50** | Vision Pure (CNN) | ~51% | Rapide, L√©ger | "Boite noire", pas d'explication, confusion sur les classes mixtes |
| **ViT** | Vision Pure (Transformer) | ~53-55% | Vue globale | Lourd √† entra√Æner |
| **Vision-LLM** | Multimodal | **> 60% (Cible)** | **Explicabilit√©**, Raisonnement, Pr√©cision sur les cas ambigus | Tr√®s lourd, lent √† l'inf√©rence |

---

## 5. Contributions et Livrables

Ce projet apporte trois contributions majeures :
1.  **Un Pipeline Unifi√©** : Une m√©thode compl√®te qui aligne l'image et le texte pour l'analyse d'√©motions.
2.  **Un Benchmark Comparatif** : Une √©valuation claire montrant les limites des mod√®les classiques (ResNet) face √† la complexit√© des √©motions compos√©es.
3.  **L'Explicabilit√© (XAI)** : Contrairement aux anciens mod√®les qui donnaient juste un chiffre, notre syst√®me explique son raisonnement, ce qui est crucial pour la confiance utilisateur (sant√©, recrutement, etc.).

### Livrables du Projet
*   üìÇ **Code Source** : Notebooks propres et organis√©s.
*   üìÑ **Rapport Scientifique** : Ce document d√©taillant toute notre d√©marche.
*   üìä **Visualisations** : Cartes de chaleur montrant les zones du visage analys√©es.
*   ü§ñ **Interface de D√©mo** (Optionnel) : Pour tester le mod√®le en direct.

---
*Ce rapport a √©t√© g√©n√©r√© pour servir de r√©f√©rence centrale au projet FER-CE. Il synth√©tise les travaux r√©alis√©s dans les diff√©rents environnements de test (`Ala's Try`, `Dhia Try`, `Sat Try`) et formalise la direction scientifique du projet.*
