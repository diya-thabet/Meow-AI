{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Vision-LLM Zero to Hero: Optimized Fine-Tuning (QLoRA)\n",
        "\n",
        "## Introduction\n",
        "Ce notebook est la solution **\"Best Ever\"** pour entra√Æner un mod√®le Vision-LLM (comme **Qwen-VL**) sur le dataset RAF-CE.\n",
        "\n",
        "### Points Cl√©s\n",
        "1.  **Vision-LLM (SOTA)** : Utilisation de Qwen-VL-Chat pour une compr√©hension visuelle et textuelle avanc√©e.\n",
        "2.  **QLoRA (4-bit)** : Optimisation m√©moire pour entra√Æner sur GPU standard.\n",
        "3.  **Data Pipeline Robuste** : Chargement des donn√©es personnalis√© pour RAF-CE.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 1: Imports & Environment Setup ---\n",
        "# Imports all necessary libraries and mounts Google Drive.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# QLoRA / Vision-LLM specific imports\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Drive already mounted or not running in Colab.\")\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 2: Configuration & Constants ---\n",
        "# Sets up paths and hyperparameters.\n",
        "\n",
        "# --- PATHS ---\n",
        "# Update this path if your folder structure changes\n",
        "DATASET_PATH = '/content/drive/MyDrive/Colab Datasets'\n",
        "IMAGE_EXTRACT_PATH = '/content/raf-ce-images'  # Where we will unzip images locally\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "BATCH_SIZE = 8 # Reduced for Vision-LLM + QLoRA\n",
        "GRADIENT_ACCUMULATION = 4 # Simulates batch size 32\n",
        "NUM_EPOCHS = 10     # As requested\n",
        "LEARNING_RATE = 2e-4 # QLoRA handles higher LRs well\n",
        "NUM_CLASSES = 15    # RAF-CE has 15 compound emotions\n",
        "\n",
        "# Emotion Labels Mapping (0-14)\n",
        "emotion_map = {\n",
        "    0: 'Happily surprised', 1: 'Happily disgusted', 2: 'Sadly fearful',\n",
        "    3: 'Sadly angry', 4: 'Sadly surprised', 5: 'Sadly disgusted',\n",
        "    6: 'Fearfully angry', 7: 'Fearfully surprised', 8: 'Fearfully disgusted',\n",
        "    9: 'Angrily surprised', 10: 'Angrily disgusted', 11: 'Disgustedly surprised',\n",
        "    12: 'Happily fearful', 13: 'Happily angry', 14: 'Happily sad'\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 3: Data Preparation Functions ---\n",
        "# Handles unzipping and parsing label files.\n",
        "\n",
        "def prepare_data(dataset_path, extract_to):\n",
        "    \"\"\"\n",
        "    1. Unzips the 'aligned.zip' file to a local folder (faster access).\n",
        "    2. Reads 'RAFCE_emolabel.txt' (Emotion labels).\n",
        "    3. Reads 'RAFCE_AUlabel.txt' (Action Unit labels).\n",
        "    4. Merges them into a single pandas DataFrame.\n",
        "    \"\"\"\n",
        "    \n",
        "    # --- Step 1: Unzip Images ---\n",
        "    zip_file = os.path.join(dataset_path, 'aligned.zip')\n",
        "    \n",
        "    if not os.path.exists(zip_file):\n",
        "        # Fallback for testing without Drive\n",
        "        print(f\"‚ö†Ô∏è Zip file not found at: {zip_file}. Checking local directory...\")\n",
        "        if os.path.exists(\"aligned.zip\"):\n",
        "             zip_file = \"aligned.zip\"\n",
        "        else:\n",
        "             print(\"‚ùå No dataset found. Please upload aligned.zip.\")\n",
        "             return None, None\n",
        "        \n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"üìÇ Extracting {zip_file}...\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(\"‚úÖ Extraction complete.\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Images already extracted (skipping unzip).\")\n",
        "\n",
        "    # Locate the specific folder inside the extracted path\n",
        "    extracted_items = os.listdir(extract_to)\n",
        "    # Heuristic: if there's only one folder inside, that's our root\n",
        "    if len(extracted_items) == 1 and os.path.isdir(os.path.join(extract_to, extracted_items[0])):\n",
        "        img_root = os.path.join(extract_to, extracted_items[0])\n",
        "    else:\n",
        "        img_root = extract_to\n",
        "        \n",
        "    print(f\"üñºÔ∏è Images located at: {img_root}\")\n",
        "\n",
        "    # --- Step 2: Parse Emotion Labels ---\n",
        "    emo_path = os.path.join(dataset_path, 'RAFCE_emolabel.txt')\n",
        "    if not os.path.exists(emo_path):\n",
        "         # Fallback check\n",
        "         if os.path.exists('RAFCE_emolabel.txt'): emo_path = 'RAFCE_emolabel.txt'\n",
        "         else: \n",
        "            print(f\"‚ùå Label file not found: {emo_path}\")\n",
        "            return None, None\n",
        "        \n",
        "    print(f\"üìñ Reading Emotions from: {emo_path}\")\n",
        "    # RAF-CE format is typically: \"filename label_index\"\n",
        "    df_emo = pd.read_csv(emo_path, sep=r'\\s+', header=None, names=['filename', 'label'])\n",
        "\n",
        "    # --- Step 3: Parse Action Unit (AU) Labels ---\n",
        "    au_path = os.path.join(dataset_path, 'RAFCE_AUlabel.txt')\n",
        "    if not os.path.exists(au_path):\n",
        "        if os.path.exists('RAFCE_AUlabel.txt'): au_path = 'RAFCE_AUlabel.txt'\n",
        "        else: print(f\"‚ö†Ô∏è AU file not found: {au_path}. Proceeding without AUs.\")\n",
        "\n",
        "    if os.path.exists(au_path):\n",
        "        print(f\"üìñ Reading AUs from: {au_path}\")\n",
        "        # RAF-CE AU format: \"filename au1 au2 ... auN\"\n",
        "        df_au = pd.read_csv(au_path, sep=r'\\s+', header=None)\n",
        "        \n",
        "        # Rename columns (0 is filename, rest are AUs)\n",
        "        au_col_names = ['filename'] + [f'AU_{i}' for i in range(1, len(df_au.columns))]\n",
        "        df_au.columns = au_col_names\n",
        "\n",
        "        # --- FIX: Sanitize AU Columns ---\n",
        "        print(\"üßπ Sanitizing AU labels to ensure numeric values...\")\n",
        "        for col in au_col_names[1:]:  # Skip filename column\n",
        "            df_au[col] = pd.to_numeric(df_au[col], errors='coerce').fillna(0.0)\n",
        "\n",
        "        # --- Step 4: Merge Data ---\n",
        "        df_merged = pd.merge(df_emo, df_au, on='filename')\n",
        "    else:\n",
        "        df_merged = df_emo\n",
        "    \n",
        "    # Add text labels for LLM\n",
        "    df_merged['label_text'] = df_merged['label'].map(emotion_map)\n",
        "    \n",
        "    # Add full image path\n",
        "    df_merged['path'] = df_merged['filename'].apply(lambda x: os.path.join(img_root, x) if not x.endswith('.jpg') else os.path.join(img_root, x.replace('.jpg', '_aligned.jpg')))\n",
        "    \n",
        "    return df_merged, img_root\n",
        "\n",
        "# Build the dataset\n",
        "df, img_root = prepare_data(DATASET_PATH, IMAGE_EXTRACT_PATH)\n",
        "print(f\"‚úÖ Data Loaded: {len(df) if df is not None else 0} samples\")\n",
        "if df is not None: display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuration du Mod√®le (QLoRA) üß†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen-VL-Chat-Int4\"  # Version optimis√©e\n",
        "\n",
        "def load_model_and_processor():\n",
        "    print(f\"üîÑ Loading {MODEL_ID}...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    \n",
        "    # BitsAndBytes Config (4-bit)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # LoRA Config\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"c_attn\", \"attn.c_proj\", \"w1\", \"w2\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    return model, processor, tokenizer\n",
        "\n",
        "model, processor, tokenizer = load_model_and_processor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Wrapper pour Vision-LLM üìÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAFCE_LLM_Dataset(Dataset):\n",
        "    def __init__(self, dataframe, processor, tokenizer):\n",
        "        self.data = dataframe\n",
        "        self.processor = processor\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        img_path = item[\"path\"]\n",
        "        label_text = item[\"label_text\"]\n",
        "        \n",
        "        # Simplified explanation generation\n",
        "        explanation = f\"The facial expression corresponds to {label_text}.\"\n",
        "\n",
        "        # Qwen-VL Prompt Format\n",
        "        prompt = f\"User: <img>{img_path}</img> Analyze the emotional state.\\nAssistant: {explanation}<|endoftext|>\"\n",
        "        \n",
        "        # Process using Qwen's processor\n",
        "        inputs = self.processor(\n",
        "            text=[prompt],\n",
        "            images=None, \n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=256,\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": inputs[\"input_ids\"].squeeze()\n",
        "        }\n",
        "\n",
        "if df is not None:\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'])\n",
        "    train_dataset = RAFCE_LLM_Dataset(train_df, processor, tokenizer)\n",
        "    val_dataset = RAFCE_LLM_Dataset(val_df, processor, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Execution üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training():\n",
        "    if df is None:\n",
        "        print(\"‚ùå No data loaded. Aborting training.\")\n",
        "        return\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./qwen_rafce_results\",\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        bf16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_total_limit=2,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "    \n",
        "    print(\"üî• Starting Training...\")\n",
        "    trainer.train()\n",
        "    model.save_pretrained(\"./best_adapter\")\n",
        "    print(\"‚úÖ Training Complete!\")\n",
        "\n",
        "# run_training()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}