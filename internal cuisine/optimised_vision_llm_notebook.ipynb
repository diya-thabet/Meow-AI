{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Vision-LLM \"Zero to Hero\": The Ultimate Workflow (QLoRA)\n",
        "\n",
        "## Introduction\n",
        "Ce notebook est la solution **d√©finitive** pour Meow-AI. Il impl√©mente un pipeline complet :\n",
        "1.  **Data Loading** : Compatible avec votre structure Drive.\n",
        "2.  **Model** : Qwen-VL-Chat optimis√© en 4-bit (QLoRA).\n",
        "3.  **Training** : Early Stopping, Logging, Optimisation VRAM.\n",
        "4.  **Validation** : M√©triques r√©elles (Accuracy, F1), Courbes de Loss, Matrice de Confusion.\n",
        "5.  **Sauvegarde** : Export automatique vers Google Drive.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 1: Imports & Environment Setup ---\n",
        "# üõ†Ô∏è INSTALLATION AUTOMATIQUE (ZERO CONFIG)\n",
        "\n",
        "print(\"‚ö° Installing optimized libraries for QLoRA...\")\n",
        "!pip install -q -U torch torchvision torchaudio\n",
        "!pip install -q -U transformers>=4.37.0 peft bitsandbytes accelerate datasets pillow scikit-learn scipy tensorboard einops tiktoken\n",
        "print(\"‚úÖ Libraries installed! Loading imports...\")\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vision-LLM & QLoRA libraries\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è Drive already mounted or local environment.\")\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 2: Configuration & Constants ---\n",
        "\n",
        "# --- PATHS & DRIVE SAVING ---\n",
        "DATASET_PATH = '/content/drive/MyDrive/Colab Datasets'\n",
        "IMAGE_EXTRACT_PATH = '/content/raf-ce-images'\n",
        "OUTPUT_DRIVE_PATH = '/content/drive/MyDrive/Meow_VisionLLM_Results/best_model' # Where to save final model\n",
        "\n",
        "# Ensure drive output dir exists\n",
        "os.makedirs(OUTPUT_DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "BATCH_SIZE = 8          # Optimized for T4 GPU (with 4-bit)\n",
        "GRAD_ACCUMULATION = 4   # Effective Batch Size = 32\n",
        "NUM_EPOCHS = 10         # Max epochs (Early Stopping will likely stop earlier)\n",
        "LEARNING_RATE = 2e-4    # Standard for QLoRA\n",
        "MAX_LENGTH = 256        # Limit token length for speed\n",
        "\n",
        "# Emotion Labels Mapping (0-14)\n",
        "emotion_map = {\n",
        "    0: 'Happily surprised', 1: 'Happily disgusted', 2: 'Sadly fearful',\n",
        "    3: 'Sadly angry', 4: 'Sadly surprised', 5: 'Sadly disgusted',\n",
        "    6: 'Fearfully angry', 7: 'Fearfully surprised', 8: 'Fearfully disgusted',\n",
        "    9: 'Angrily surprised', 10: 'Angrily disgusted', 11: 'Disgustedly surprised',\n",
        "    12: 'Happily fearful', 13: 'Happily angry', 14: 'Happily sad'\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Config Loaded. Results will be saved to: {OUTPUT_DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 3: Data Loading Pipeline ---\n",
        "\n",
        "def prepare_data(dataset_path, extract_to):\n",
        "    # 1. Unzip Logic\n",
        "    zip_file = os.path.join(dataset_path, 'aligned.zip')\n",
        "    if not os.path.exists(zip_file):\n",
        "        print(f\"‚ö†Ô∏è Zip not found at {zip_file}. Checking current dir...\")\n",
        "        if os.path.exists('aligned.zip'): zip_file = 'aligned.zip'\n",
        "        else: \n",
        "            print(\"‚ùå No aligned.zip found!\")\n",
        "            return None, None\n",
        "\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"üìÇ Unzipping to {extract_to}...\")\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "    \n",
        "    # 2. Find Image Root\n",
        "    extracted_items = os.listdir(extract_to)\n",
        "    if len(extracted_items) == 1 and os.path.isdir(os.path.join(extract_to, extracted_items[0])):\n",
        "        img_root = os.path.join(extract_to, extracted_items[0])\n",
        "    else:\n",
        "        img_root = extract_to\n",
        "\n",
        "    # 3. Load Labels\n",
        "    # Attempt to find label file in Data Path OR extracted path\n",
        "    emo_path = os.path.join(dataset_path, 'RAFCE_emolabel.txt')\n",
        "    if not os.path.exists(emo_path):\n",
        "        emo_path = os.path.join(img_root, '../RAFCE_emolabel.txt') # Common structure\n",
        "    \n",
        "    if not os.path.exists(emo_path) and os.path.exists('RAFCE_emolabel.txt'):\n",
        "        emo_path = 'RAFCE_emolabel.txt'\n",
        "        \n",
        "    if not os.path.exists(emo_path):\n",
        "        print(\"‚ùå Labels not found!\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"üìñ Loading labels from {emo_path}\")\n",
        "    df = pd.read_csv(emo_path, sep=r'\\s+', header=None, names=['filename', 'label'])\n",
        "    \n",
        "    # 4. Process Attributes\n",
        "    df['label_text'] = df['label'].map(emotion_map)\n",
        "    df['path'] = df['filename'].apply(lambda x: os.path.join(img_root, x) if not x.endswith('.jpg') else os.path.join(img_root, x.replace('.jpg', '_aligned.jpg')))\n",
        "    \n",
        "    # Filter missing images\n",
        "    df = df[df['path'].apply(os.path.exists)]\n",
        "    \n",
        "    return df, img_root\n",
        "\n",
        "df, img_root = prepare_data(DATASET_PATH, IMAGE_EXTRACT_PATH)\n",
        "print(f\"‚úÖ Total Images available: {len(df) if df is not None else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 4: QLoRA Model Setup ---\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen-VL-Chat-Int4\" # Optimized Base Model\n",
        "\n",
        "def get_model():\n",
        "    print(f\"üîÑ Loading {MODEL_ID}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    \n",
        "    # 4-bit Quantization Config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    # Load Model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # LoRA Config\n",
        "    peft_config = LoraConfig(\n",
        "        r=16, \n",
        "        lora_alpha=32, \n",
        "        target_modules=[\"c_attn\", \"attn.c_proj\", \"w1\", \"w2\"],\n",
        "        lora_dropout=0.05, \n",
        "        bias=\"none\", \n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model, processor, tokenizer\n",
        "\n",
        "model, processor, tokenizer = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 5: Custom Dataset & Prompt Engineering ---\n",
        "\n",
        "class RAFCE_QwenDataset(Dataset):\n",
        "    def __init__(self, data, processor):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        \n",
        "        # Optimized Prompt: Short & Direct\n",
        "        prompt = f\"User: <img>{item['path']}</img> Analyze facial cues. What is the compound emotion?\\nAssistant: The emotion is {item['label_text']}.<|endoftext|>\"\n",
        "        \n",
        "        inputs = self.processor(\n",
        "            text=[prompt],\n",
        "            images=None,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LENGTH,\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": inputs[\"input_ids\"].squeeze() # For Causal LM, labels = inputs\n",
        "        }\n",
        "\n",
        "# Split Data\n",
        "train_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
        "train_dataset = RAFCE_QwenDataset(train_df, processor)\n",
        "val_dataset = RAFCE_QwenDataset(val_df, processor)\n",
        "\n",
        "print(f\"üìä Train Set: {len(train_df)} | Val Set: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 6: Training Setup with Early Stopping ---\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    bf16=True,                           # Faster on T4/A10\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",         # Evaluate every X steps\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,                  # Keep only best checkpoint\n",
        "    load_best_model_at_end=True,         # Important!\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=[\"tensorboard\"]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 evals\n",
        ")\n",
        "\n",
        "print(\"üî• Starting Optimized Training...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training Finished (Best Model Loaded).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 7: Visualisation (Loss Curves) ---\n",
        "\n",
        "history = pd.DataFrame(trainer.state.log_history)\n",
        "train_loss = history[history['loss'].notna()][['step', 'loss']]\n",
        "val_loss = history[history['eval_loss'].notna()][['step', 'eval_loss']]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss['step'], train_loss['loss'], label='Training Loss')\n",
        "plt.plot(val_loss['step'], val_loss['eval_loss'], label='Validation Loss', marker='o')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 8: FULL EVALUATION (Confusion Matrix & Metrics) ---\n",
        "# We run inference on the Validation Set to get real Accuracy/F1\n",
        "\n",
        "def evaluate_performance(model, val_df, processor, tokenizer):\n",
        "    print(\"üïµÔ∏è Running Full Evaluation on Validation Set...\")\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
        "            # Prompt for inference (without answer)\n",
        "            prompt = f\"User: <img>{row['path']}</img> Analyze facial cues. What is the compound emotion?\\nAssistant:\"\n",
        "            \n",
        "            inputs = processor(text=[prompt], return_tensors=\"pt\").to(device)\n",
        "            \n",
        "            # Generate Answer\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=20,\n",
        "                do_sample=False  # Deterministic\n",
        "            )\n",
        "            output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "            \n",
        "            # Extract Prediction\n",
        "            prediction = output_text.split(\"Assistant:\")[-1].strip().split(\".\")[0]\n",
        "            \n",
        "            # Store\n",
        "            true_labels.append(row['label_text'])\n",
        "            # Simple matching: check if expected label is in output\n",
        "            # (Robust matching would be better, but this works for \"The emotion is X\" format)\n",
        "            matched = False\n",
        "            for label in emotion_map.values():\n",
        "                if label.lower() in prediction.lower():\n",
        "                    pred_labels.append(label)\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                pred_labels.append(\"Unknown\")\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    f1 = f1_score(true_labels, pred_labels, average='macro')\n",
        "    \n",
        "    print(f\"\\nüèÜ Final Accuracy: {acc:.4f}\")\n",
        "    print(f\"‚≠ê Final Macro F1: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "    \n",
        "    # Confusion Matrix Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=list(emotion_map.values()))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_map.values(), yticklabels=emotion_map.values(), cmap='Blues')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Run Evaluation (Takes some time but worth it)\n",
        "evaluate_performance(model, val_df, processor, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CELL 9: Save Best Model to Drive ---\n",
        "print(f\"üíæ Saving Adapter to {OUTPUT_DRIVE_PATH}...\")\n",
        "model.save_pretrained(OUTPUT_DRIVE_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_DRIVE_PATH)\n",
        "processor.save_pretrained(OUTPUT_DRIVE_PATH)\n",
        "print(\"‚úÖ Saved successfully! You can load it later using PeftModel.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}